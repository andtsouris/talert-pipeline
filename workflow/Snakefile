import yaml
import glob
import os
import pandas as pd


# Making output directories
def create_dirs(my_data_path):
    result_dirs=['fastqc_raw', 'trimmed_reads', 'fastqc_trimmed', 'salmon_index', 'salmon_quant']
    os.makedirs(f'{my_data_path}/logs', exist_ok=True)
    for dir in result_dirs:
        os.makedirs(f'{my_data_path}/Results/{dir}', exist_ok=True)

create_dirs(config["data_path"])

# Get sample names
samples = [os.path.basename(x).replace('_R1.fastq.gz', '') for x in glob.glob(f'{config["data_path"]}fastq/*_R1.fastq.gz')]

# Get global variables
data_dir = config["data_path"]
fastqc=config["fastqc"]
trimmomatic = config["trimmomatic"]
salmon = config["salmon"]

# Global rule
rule all:
    input:
        expand(data_dir + "Results/fastqc_raw/{sample}_R{read}_fastqc.{ext}", 
        ext=["html", "zip"], sample=samples, read=[1, 2]),
        expand(data_dir + "Results/trimmed_reads/{sample}_R{read}_trimmed.fastq.gz",
        sample=samples, read=[1, 2]),
        expand(data_dir + "Results/fastqc_trimmed/{sample}_R{read}_trimmed_fastqc.{ext}",
        ext=["html", "zip"], sample=samples, read=[1, 2]),
        # Skipping the salmon indexing step
        # data_dir + "Results/salmon_index/decoys.txt",
        # data_dir + "Results/salmon_index/gentrome.fa.gz"#,
        #data_dir + "Results/salmon_index/seq.bin",
        #data_dir + "Results/salmon_index/pre_indexing.log"
        expand(data_dir + "Results/salmon_quant/{sample}_salmon_quant.sf", sample=samples),
        f"{data_dir}Results/" "salmon_quant_all.csv"
####################


# FastQC on raw file
qc1_in_dir = f"{data_dir}fastq/"
qc1_out_dir = f"{data_dir}Results/fastqc_raw/"

rule fastqc_raw:
    input:
        raw_1 = qc1_in_dir + "{sample}_R1.fastq.gz",
        raw_2 = qc1_in_dir + "{sample}_R2.fastq.gz"
    output:
        html1 = qc1_out_dir + "{sample}_R1_fastqc.html",
        zip1 = qc1_out_dir + "{sample}_R1_fastqc.zip",
        html2 = qc1_out_dir + "{sample}_R2_fastqc.html",
        zip2 = qc1_out_dir + "{sample}_R2_fastqc.zip"
    resources:
        mem_mb = 8000
    log:
        data_dir + "logs/fastqc_raw_{sample}.log"
    shell:
        fastqc + " {input.raw_1} {input.raw_2} -o {qc1_out_dir} > {log} 2>&1"
####################        

# Trimmomatic
trim_in_dir = f"{data_dir}fastq/"
trim_out_dir = f"{data_dir}Results/trimmed_reads/"

rule trimming:
    input:
        raw_1 = trim_in_dir + "{sample}_R1.fastq.gz",
        raw_2 = trim_in_dir + "{sample}_R2.fastq.gz"
    output:
        trimmed_1 = trim_out_dir + "{sample}_R1_trimmed.fastq.gz",
        trimmed_2 = trim_out_dir + "{sample}_R2_trimmed.fastq.gz"
    params:
        phred_score = config["phred_score"],
        adapters = config["adapters"],
        leading = config["leading_score"],
        trailing = config["trailing_score"],
        sliding_window = config["sliding_window_values"],
        minlength = config["minimum_length"]
    resources:
        mem_mb = "8G" #16G
    log:
        data_dir + "logs/trimmomatic_{sample}.log"
    benchmark:
        data_dir + "logs/trimmomatic_{sample}.benchmark"
    shell:
        trimmomatic + """ PE -threads 2 \
        -{params.phred_score} \
        {input.raw_1} {input.raw_2} \
        {output.trimmed_1} /dev/null {output.trimmed_2} /dev/null \
        ILLUMINACLIP:{params.adapters}:2:30:10 \
        LEADING:{params.leading} TRAILING:{params.trailing} \
        SLIDINGWINDOW:{params.sliding_window} \
        MINLEN:{params.minlength} > {log} 2>&1
        """
####################               

# FastQC on trimmed reads
qc2_in_dir = f"{data_dir}Results/trimmed_reads/"
qc2_out_dir = f"{data_dir}Results/fastqc_trimmed/"

rule fastqc_trimmed:
    input:
        trimmed_1 = qc2_in_dir + "{sample}_R1_trimmed.fastq.gz",
        trimmed_2 = qc2_in_dir + "{sample}_R2_trimmed.fastq.gz"
    output:
        html1 = qc2_out_dir + "{sample}_R1_trimmed_fastqc.html",
        zip1 = qc2_out_dir + "{sample}_R1_trimmed_fastqc.zip",
        html2 = qc2_out_dir + "{sample}_R2_trimmed_fastqc.html",
        zip2 = qc2_out_dir + "{sample}_R2_trimmed_fastqc.zip"
    resources:
        mem_mb = "8G"
    log:
        data_dir + "logs/fastqc_trimmed_{sample}.log"
    shell:
        fastqc + " {input.trimmed_1} {input.trimmed_2} -o {qc2_out_dir} > {log} 2>&1"
#

# # Salmon index - 
# !!!! THERE IS AN ISSU WITH THIS STEP, 
# !!!! IT DOESN'T CREATE THE seq_bin AND indexlog outputs
# !!!! The script works without it 
# as I'm using the seq_bin and indexlog of Tomnoy and the same refs
# Should be fixed if we want to use other reference sequences
if False:   
    '''
    # ref_dir = f"{config['ref_path']}"
    # index_out_dir = f"{data_dir}Results/salmon_index/"
    # salmon_index = config["salmon_index"]
    # rule salmon_index:
    #     input:
    #         ref_genome = ref_dir + config["ref_genome"],
    #         ref_transcriptome = ref_dir + config["ref_transcriptome"]
    #     output:
    #         decoy = index_out_dir + "decoys.txt",
    #         gentrome = index_out_dir + "gentrome.fa.gz"   #,
    #         #seqbin = protected(index_out_dir + "seq.bin"),
    #         #indexlog = protected(index_out_dir + "pre_indexing.log")
    #     shell:
    #         """
    #         # Extract the names of the genome targets
    #         grep "^>" <(gunzip -c {input.ref_genome}) | cut -d " " -f 1 > {output.decoy}
    #         sed -i.bak -e 's/>//g' {output.decoy}

    #         # Concatenate trascriptome and genome
    #         cat {input.ref_transcriptome} {input.ref_genome} > {output.gentrome}

    #         # Perform indexing
    #         {salmon} index -p 12 -t {output.gentrome} -d {output.decoy} -i {salmon_index}
    #         """
    # '''
#

# Salmon quantification
quant_in = trim_out_dir
quant_out = f"{data_dir}Results/salmon_quant/"
salmon_index = config["salmon_index"]

rule salmon_quantication:
    input:
        trimmed_1 = quant_in + "{sample}_R1_trimmed.fastq.gz",
        trimmed_2 = quant_in + "{sample}_R2_trimmed.fastq.gz"
        # seqbin = config["salmon_index"] + "seq.bin",
        # dummy = expand(quant_out + "{{sample}}/qc/fastqc/trimmed/{{sample}}_R{read}_trimmed_fastqc.html", read=[1,2])
    output:
        quant_out + "{sample}_salmon_quant.sf"
    params:
        option_salmon = config["option_salmon"]
    threads: 8
    resources: 
        mem_mb= "8G"
    log:
        data_dir + "logs/salmon_quant_{sample}.log"
    shell:
        """
        # Run salmon
        {salmon} quant -i {salmon_index} -l A -1 {input.trimmed_1} -2 {input.trimmed_2} \
        -o {quant_out}{wildcards.sample} \
        -p {threads} {params.option_salmon} > {log} 2>&1
        
        # Rename quant.sf
        mv {quant_out}{wildcards.sample}/quant.sf {output}
        """
#

# Merge salmon
merge_out = f"{data_dir}Results/"

rule merge_salmon_sf:
                input: 
                    expand(quant_out + "{sample}_salmon_quant.sf", sample=samples)
                output: 
                    merge_out + "salmon_quant_all.csv"
                run:  
                    # Get the matching files paths
                    sf_files = input
                    
                    # Merge the sf files
                    merged_df = pd.DataFrame()
                    for i in sf_files:
                        count = pd.read_csv(i, delimiter="\t", index_col=0)
                        column_name = os.path.basename(i).split("_")[0]
                        count = count.rename(columns={"TPM": column_name})
                        merged_df[os.path.basename(i).split("_")[0]] = count[os.path.basename(i).split("_")[0]]

                    # Fix the index id name
                    merged_df.index.name = "ENSID"

                    # Save the file to csv
                    merged_df.to_csv(output[0])
#